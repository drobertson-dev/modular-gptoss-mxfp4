[workspace]
authors = ["Modular <hello@modular.com>"]
channels = ["conda-forge", "https://conda.modular.com/max-nightly/"]
description = "An example of a custom model architecture served by MAX"
name = "Custom MAX Model Serving"
platforms = ["osx-arm64", "linux-aarch64", "linux-64"]
version = "0.1.0"

[tasks]
# Upstream example (kept for reference).
generate-qwen2 = "python -m max.entrypoints.pipelines generate --custom-architectures qwen2 --model-path Qwen/Qwen2.5-0.5B-Instruct --max-length 10000 --prompt 'Why is the sky blue?'"
serve-qwen2 = "python -m max.entrypoints.pipelines serve --custom-architectures qwen2 --model-path Qwen/Qwen2.5-0.5B-Instruct"

generate = "USE_EXPERIMENTAL_KERNELS=1 python -m max.entrypoints.pipelines generate --custom-architectures $PIXI_PROJECT_ROOT/gpt_oss_mxfp4 --model-path openai/gpt-oss-20b --quantization-encoding bfloat16 --devices gpu --max-length 10000 --prompt 'Why is the sky blue?'"
generate-smoke = "USE_EXPERIMENTAL_KERNELS=1 python -m max.entrypoints.pipelines generate --custom-architectures $PIXI_PROJECT_ROOT/gpt_oss_mxfp4 --model-path openai/gpt-oss-20b --quantization-encoding bfloat16 --devices gpu --max-length 2048 --prompt 'Say hello in one sentence.'"
generate-smoke-prof = "USE_EXPERIMENTAL_KERNELS=1 python -m max.entrypoints.pipelines generate --custom-architectures $PIXI_PROJECT_ROOT/gpt_oss_mxfp4 --model-path openai/gpt-oss-20b --quantization-encoding bfloat16 --devices gpu --max-length 2048 --prompt 'Say hello in one sentence.' --gpu-profiling detailed"
generate-v3-smoke = "USE_EXPERIMENTAL_KERNELS=1 python -m max.entrypoints.pipelines generate --custom-architectures $PIXI_PROJECT_ROOT/gpt_oss_mxfp4_v3 --no-use-legacy-module --model-path openai/gpt-oss-20b --quantization-encoding bfloat16 --devices gpu --chat-template $PIXI_PROJECT_ROOT/gpt_oss_mxfp4_v3/templates/gpt-oss-20b.chat_template.jinja --max-length 2048 --prompt 'Say hello in one sentence.'"
serve = "USE_EXPERIMENTAL_KERNELS=1 python -m max.entrypoints.pipelines serve --custom-architectures $PIXI_PROJECT_ROOT/gpt_oss_mxfp4 --model-path openai/gpt-oss-20b --quantization-encoding bfloat16 --devices gpu --chat-template $PIXI_PROJECT_ROOT/gpt_oss_mxfp4/templates/gpt-oss-20b.chat_template.jinja --enable-in-flight-batching --no-enable-prefix-caching --pretty-print-config"
serve-prof = "USE_EXPERIMENTAL_KERNELS=1 python -m max.entrypoints.pipelines serve --custom-architectures $PIXI_PROJECT_ROOT/gpt_oss_mxfp4 --model-path openai/gpt-oss-20b --quantization-encoding bfloat16 --devices gpu --chat-template $PIXI_PROJECT_ROOT/gpt_oss_mxfp4/templates/gpt-oss-20b.chat_template.jinja --enable-in-flight-batching --no-enable-prefix-caching --pretty-print-config --gpu-profiling detailed"
serve-20b = "USE_EXPERIMENTAL_KERNELS=1 python -m max.entrypoints.pipelines serve --custom-architectures $PIXI_PROJECT_ROOT/gpt_oss_mxfp4 --model-path openai/gpt-oss-20b --quantization-encoding bfloat16 --devices gpu --chat-template $PIXI_PROJECT_ROOT/gpt_oss_mxfp4/templates/gpt-oss-20b.chat_template.jinja --enable-in-flight-batching --no-enable-prefix-caching --pretty-print-config"
serve-20b-prof = "USE_EXPERIMENTAL_KERNELS=1 python -m max.entrypoints.pipelines serve --custom-architectures $PIXI_PROJECT_ROOT/gpt_oss_mxfp4 --model-path openai/gpt-oss-20b --quantization-encoding bfloat16 --devices gpu --chat-template $PIXI_PROJECT_ROOT/gpt_oss_mxfp4/templates/gpt-oss-20b.chat_template.jinja --enable-in-flight-batching --no-enable-prefix-caching --pretty-print-config --gpu-profiling detailed"
serve-20b-v3 = "USE_EXPERIMENTAL_KERNELS=1 python -m max.entrypoints.pipelines serve --custom-architectures $PIXI_PROJECT_ROOT/gpt_oss_mxfp4_v3 --no-use-legacy-module --model-path openai/gpt-oss-20b --quantization-encoding bfloat16 --devices gpu --chat-template $PIXI_PROJECT_ROOT/gpt_oss_mxfp4_v3/templates/gpt-oss-20b.chat_template.jinja --enable-in-flight-batching --no-enable-prefix-caching --pretty-print-config"
serve-20b-v3-prof = "USE_EXPERIMENTAL_KERNELS=1 python -m max.entrypoints.pipelines serve --custom-architectures $PIXI_PROJECT_ROOT/gpt_oss_mxfp4_v3 --no-use-legacy-module --model-path openai/gpt-oss-20b --quantization-encoding bfloat16 --devices gpu --chat-template $PIXI_PROJECT_ROOT/gpt_oss_mxfp4_v3/templates/gpt-oss-20b.chat_template.jinja --enable-in-flight-batching --no-enable-prefix-caching --pretty-print-config --gpu-profiling detailed"
serve-120b = "USE_EXPERIMENTAL_KERNELS=1 python -m max.entrypoints.pipelines serve --custom-architectures $PIXI_PROJECT_ROOT/gpt_oss_mxfp4 --model-path openai/gpt-oss-120b --quantization-encoding bfloat16 --devices gpu --chat-template $PIXI_PROJECT_ROOT/gpt_oss_mxfp4/templates/gpt-oss-120b.chat_template.jinja --device-memory-utilization 0.95 --max-batch-size 64 --max-length 4096 --enable-in-flight-batching --enable-chunked-prefill --prefill-chunk-size 2048 --no-enable-prefix-caching --pretty-print-config"
serve-120b-prof = "USE_EXPERIMENTAL_KERNELS=1 python -m max.entrypoints.pipelines serve --custom-architectures $PIXI_PROJECT_ROOT/gpt_oss_mxfp4 --model-path openai/gpt-oss-120b --quantization-encoding bfloat16 --devices gpu --chat-template $PIXI_PROJECT_ROOT/gpt_oss_mxfp4/templates/gpt-oss-120b.chat_template.jinja --device-memory-utilization 0.95 --max-batch-size 64 --max-length 4096 --enable-in-flight-batching --enable-chunked-prefill --prefill-chunk-size 2048 --no-enable-prefix-caching --pretty-print-config --gpu-profiling detailed"

# NOTE: `max benchmark` expects a running `pixi run serve-*` instance.
# `SHAREGPT_PATH` defaults to `/workspace/ShareGPT_V3_unfiltered_cleaned_split.json` via the relative fallback below.
benchmark-20b-sharegpt-500-r32 = "max benchmark --model openai/gpt-oss-20b --backend modular --endpoint /v1/chat/completions --dataset-name sharegpt --num-prompts 500 --seed 0 --request-rate 32 --dataset-path ${SHAREGPT_PATH:-$PIXI_PROJECT_ROOT/../../../ShareGPT_V3_unfiltered_cleaned_split.json}"
benchmark-20b-sharegpt-500-r32-seed123 = "max benchmark --model openai/gpt-oss-20b --backend modular --endpoint /v1/chat/completions --dataset-name sharegpt --num-prompts 500 --seed 123 --request-rate 32 --dataset-path ${SHAREGPT_PATH:-$PIXI_PROJECT_ROOT/../../../ShareGPT_V3_unfiltered_cleaned_split.json}"
benchmark-20b-sharegpt-500-r16 = "max benchmark --model openai/gpt-oss-20b --backend modular --endpoint /v1/chat/completions --dataset-name sharegpt --num-prompts 500 --seed 0 --request-rate 16 --dataset-path ${SHAREGPT_PATH:-$PIXI_PROJECT_ROOT/../../../ShareGPT_V3_unfiltered_cleaned_split.json}"
benchmark-120b-sharegpt-100-r32-seed123 = "max benchmark --model openai/gpt-oss-120b --backend modular --endpoint /v1/chat/completions --dataset-name sharegpt --num-prompts 100 --seed 123 --request-rate 32 --dataset-path ${SHAREGPT_PATH:-$PIXI_PROJECT_ROOT/../../../ShareGPT_V3_unfiltered_cleaned_split.json}"
benchmark-20b-baseline = "max benchmark --model openai/gpt-oss-20b --backend modular --endpoint /v1/chat/completions --dataset-name sharegpt --num-prompts 500 --seed 123 --request-rate 32 --dataset-path ${SHAREGPT_PATH:-$PIXI_PROJECT_ROOT/../../../ShareGPT_V3_unfiltered_cleaned_split.json}"
benchmark-120b-baseline = "max benchmark --model openai/gpt-oss-120b --backend modular --endpoint /v1/chat/completions --dataset-name sharegpt --num-prompts 100 --seed 123 --request-rate 32 --dataset-path ${SHAREGPT_PATH:-$PIXI_PROJECT_ROOT/../../../ShareGPT_V3_unfiltered_cleaned_split.json}"
mxfp4-scale-swizzle-test = "PYTHONPATH=$PIXI_PROJECT_ROOT pytest $PIXI_PROJECT_ROOT/tests/test_mxfp4_scale_swizzle_reference.py -q"
mxfp4-value-layout-test = "PYTHONPATH=$PIXI_PROJECT_ROOT pytest $PIXI_PROJECT_ROOT/tests/test_mxfp4_value_layout_hopper.py -q"
mxfp4-py-tests = { depends-on = [
  "mxfp4-scale-swizzle-test",
  "mxfp4-value-layout-test",
] }

# Mojo tests (use Mojo runner; include `../custom_ops` so `kernels.*` resolves).
mxfp4-mojo-quant-layout-test = "mojo run -I $PIXI_PROJECT_ROOT/../custom_ops $PIXI_PROJECT_ROOT/tests/mojo/test_mxfp4_quant_layout.mojo"
mxfp4-mojo-swiglu-math-test = "mojo run -I $PIXI_PROJECT_ROOT/../custom_ops $PIXI_PROJECT_ROOT/tests/mojo/test_mxfp4_swiglu_math.mojo"
mxfp4-mojo-tests = { depends-on = [
  "mxfp4-mojo-quant-layout-test",
  "mxfp4-mojo-swiglu-math-test",
] }

# Deprecated SM90-only Mojo tests (legacy kernels kept for reference).
deprecated-mxfp4-mojo-sm90-clean-tile-test = "mojo run -I $PIXI_PROJECT_ROOT/../custom_ops $PIXI_PROJECT_ROOT/tests/mojo/deprecated/test_mxfp4_sm90_clean_tile_wgmma.mojo"
deprecated-mxfp4-mojo-sm90-moe-test = "mojo run -I $PIXI_PROJECT_ROOT/../custom_ops $PIXI_PROJECT_ROOT/tests/mojo/deprecated/test_mxfp4_moe_sm90.mojo"
mxfp4-moe-bench = "mojo run -I $PIXI_PROJECT_ROOT/../custom_ops $PIXI_PROJECT_ROOT/tests/mojo/bench_mxfp4_moe_ops.mojo"
mxfp4-moe-bench-20b = "mojo run -I $PIXI_PROJECT_ROOT/../custom_ops $PIXI_PROJECT_ROOT/tests/mojo/bench_mxfp4_moe_ops.mojo --20b"
mxfp4-moe-bench-120b = "mojo run -I $PIXI_PROJECT_ROOT/../custom_ops $PIXI_PROJECT_ROOT/tests/mojo/bench_mxfp4_moe_ops.mojo --120b"
mxfp4-moe-bench-20b-tokens64 = "mojo run -I $PIXI_PROJECT_ROOT/../custom_ops $PIXI_PROJECT_ROOT/tests/mojo/bench_mxfp4_moe_ops.mojo --20b --tokens64"
mxfp4-moe-bench-120b-tokens64 = "mojo run -I $PIXI_PROJECT_ROOT/../custom_ops $PIXI_PROJECT_ROOT/tests/mojo/bench_mxfp4_moe_ops.mojo --120b --tokens64"
mxfp4-moe-bench-vllm = "mojo run -I $PIXI_PROJECT_ROOT/../custom_ops $PIXI_PROJECT_ROOT/tests/mojo/bench_mxfp4_moe_ops_vllm.mojo"
mxfp4-moe-bench-vllm-20b = "mojo run -I $PIXI_PROJECT_ROOT/../custom_ops $PIXI_PROJECT_ROOT/tests/mojo/bench_mxfp4_moe_ops_vllm.mojo --20b"
mxfp4-moe-bench-vllm-120b = "mojo run -I $PIXI_PROJECT_ROOT/../custom_ops $PIXI_PROJECT_ROOT/tests/mojo/bench_mxfp4_moe_ops_vllm.mojo --120b"
mxfp4-ogs-bench = "PYTHONPATH=$PIXI_PROJECT_ROOT MXFP4_V3_TRITON_KERNELS_PATH=/workspace/triton/python/triton_kernels python $PIXI_PROJECT_ROOT/tests/bench_mxfp4_moe_ops_vllm_ogs.py"
mxfp4-ogs-bench-tokens64 = "PYTHONPATH=$PIXI_PROJECT_ROOT MXFP4_V3_TRITON_KERNELS_PATH=/workspace/triton/python/triton_kernels python $PIXI_PROJECT_ROOT/tests/bench_mxfp4_moe_ops_vllm_ogs.py --tokens 64"
mxfp4-ogs-bench-tokens256 = "PYTHONPATH=$PIXI_PROJECT_ROOT MXFP4_V3_TRITON_KERNELS_PATH=/workspace/triton/python/triton_kernels python $PIXI_PROJECT_ROOT/tests/bench_mxfp4_moe_ops_vllm_ogs.py --tokens 256"
mxfp4-ogs-bench-tokens1024 = "PYTHONPATH=$PIXI_PROJECT_ROOT MXFP4_V3_TRITON_KERNELS_PATH=/workspace/triton/python/triton_kernels python $PIXI_PROJECT_ROOT/tests/bench_mxfp4_moe_ops_vllm_ogs.py --tokens 1024"
mxfp4-moe-bench-e2e = "PYTHONPATH=$PIXI_PROJECT_ROOT USE_EXPERIMENTAL_KERNELS=1 python $PIXI_PROJECT_ROOT/tests/bench_mxfp4_moe_ops_end_to_end.py"
mxfp4-moe-bench-e2e-tokens64 = "PYTHONPATH=$PIXI_PROJECT_ROOT USE_EXPERIMENTAL_KERNELS=1 python $PIXI_PROJECT_ROOT/tests/bench_mxfp4_moe_ops_end_to_end.py --tokens 64"
mxfp4-moe-bench-e2e-tokens256 = "PYTHONPATH=$PIXI_PROJECT_ROOT USE_EXPERIMENTAL_KERNELS=1 python $PIXI_PROJECT_ROOT/tests/bench_mxfp4_moe_ops_end_to_end.py --tokens 256"
mxfp4-moe-bench-e2e-tokens1024 = "PYTHONPATH=$PIXI_PROJECT_ROOT USE_EXPERIMENTAL_KERNELS=1 python $PIXI_PROJECT_ROOT/tests/bench_mxfp4_moe_ops_end_to_end.py --tokens 1024"
mxfp4-grouped-bench = "mojo run -I $PIXI_PROJECT_ROOT/../custom_ops $PIXI_PROJECT_ROOT/tests/mojo/bench_mxfp4_grouped_matmul.mojo"
mxfp4-grouped-bench-20b = "mojo run -I $PIXI_PROJECT_ROOT/../custom_ops $PIXI_PROJECT_ROOT/tests/mojo/bench_mxfp4_grouped_matmul.mojo --20b"
mxfp4-grouped-bench-120b = "mojo run -I $PIXI_PROJECT_ROOT/../custom_ops $PIXI_PROJECT_ROOT/tests/mojo/bench_mxfp4_grouped_matmul.mojo --120b"
mxfp4-eager-smoke = "USE_EXPERIMENTAL_KERNELS=1 python $PIXI_PROJECT_ROOT/gpt_oss_mxfp4_v3/eager_mxfp4_moe_smoke.py"
mxfp4-ogs-eager-smoke = "USE_EXPERIMENTAL_KERNELS=1 PYTHONPATH=$PIXI_PROJECT_ROOT MXFP4_V3_TRITON_KERNELS_PATH=/workspace/triton/python/triton_kernels python $PIXI_PROJECT_ROOT/gpt_oss_mxfp4_v3/eager_mxfp4_moe_ogs_smoke.py"

# Debug/profiling helpers (CLI-only; see `.agents/skills/debugging-mojo/`).
setup-cuda-gdb = "bash -lc 'set -euo pipefail; echo \"mojo: $(mojo --version)\"; echo \"cuda-gdb: $(cuda-gdb --version | head -n 1)\"; echo \"nsys: $(nsys --version)\"; echo \"ncu: $(ncu --version | head -n 1)\"; nvidia-smi -L'"
debug-moe-bench-cuda-gdb = "mojo debug --cuda-gdb --break-on-launch -I $PIXI_PROJECT_ROOT/../custom_ops $PIXI_PROJECT_ROOT/tests/mojo/bench_mxfp4_moe_ops.mojo"
profile-generate-smoke-20b-nsys = "bash -lc \"set -euo pipefail; mkdir -p \\\"$PIXI_PROJECT_ROOT/profiles\\\"; nsys profile -o \\\"$PIXI_PROJECT_ROOT/profiles/generate_smoke_20b\\\" --trace=cuda,nvtx,osrt python -m max.entrypoints.pipelines generate --custom-architectures \\\"$PIXI_PROJECT_ROOT/gpt_oss_mxfp4\\\" --model-path openai/gpt-oss-20b --devices gpu:0 --device-memory-utilization 0.95 --chat-template \\\"$PIXI_PROJECT_ROOT/gpt_oss_mxfp4/templates/gpt-oss-20b.chat_template.jinja\\\" --max-length 2048 --num-warmups 1 --gpu-profiling on --prompt 'Say hello in one sentence.'\""
profile-eager-mxfp4-nsys = "bash -lc \"set -euo pipefail; mkdir -p \\\"$PIXI_PROJECT_ROOT/profiles\\\"; MODULAR_ENABLE_PROFILING=detailed nsys profile -o \\\"$PIXI_PROJECT_ROOT/profiles/eager_mxfp4_moe\\\" --trace=cuda,nvtx,osrt python \\\"$PIXI_PROJECT_ROOT/gpt_oss_mxfp4_v3/eager_mxfp4_moe_smoke.py\\\"\""
profile-eager-mxfp4-ncu = "bash -lc \"set -euo pipefail; mkdir -p \\\"$PIXI_PROJECT_ROOT/profiles\\\"; sudo env MODULAR_HOME=\\\"$MODULAR_HOME\\\" /usr/local/cuda/bin/ncu --target-processes all --set full --force-overwrite -o \\\"$PIXI_PROJECT_ROOT/profiles/eager_mxfp4_moe_ncu\\\" \\\"$PIXI_PROJECT_ROOT/.pixi/envs/default/bin/python\\\" \\\"$PIXI_PROJECT_ROOT/gpt_oss_mxfp4_v3/eager_mxfp4_moe_smoke.py\\\"\""
mxfp4-moe-kbench = "bash -lc \"set -euo pipefail; cd \\\"$PIXI_PROJECT_ROOT/../../..\\\"; export KERNEL_BENCHMARKS_ROOT=\\\"$PWD/max/kernels/benchmarks\\\"; ./bazelw run //max/kernels/benchmarks/autotune:kbench -- \\\"$PIXI_PROJECT_ROOT/benchmarks/mxfp4_moe_kbench.yaml\\\" --build-opts \\\"-I $PIXI_PROJECT_ROOT/../custom_ops\\\" --output mxfp4_moe_kbench\""
profile-moe-bench-nsys = "bash -lc \"set -euo pipefail; mkdir -p \\\"$PIXI_PROJECT_ROOT/profiles\\\"; nsys profile -o \\\"$PIXI_PROJECT_ROOT/profiles/moe_bench\\\" --trace=cuda,nvtx,osrt mojo run -I \\\"$PIXI_PROJECT_ROOT/../custom_ops\\\" \\\"$PIXI_PROJECT_ROOT/tests/mojo/bench_mxfp4_moe_ops.mojo\\\"\""
profile-moe-bench-ncu = "bash -lc \"set -euo pipefail; mkdir -p \\\"$PIXI_PROJECT_ROOT/profiles\\\"; sudo env MODULAR_HOME=\\\"$MODULAR_HOME\\\" /usr/local/cuda/bin/ncu --target-processes all --set full --force-overwrite -o \\\"$PIXI_PROJECT_ROOT/profiles/moe_bench_ncu\\\" \\\"$PIXI_PROJECT_ROOT/.pixi/envs/default/bin/mojo\\\" run -I \\\"$PIXI_PROJECT_ROOT/../custom_ops\\\" \\\"$PIXI_PROJECT_ROOT/tests/mojo/bench_mxfp4_moe_ops.mojo\\\"\""
profile-moe-bench-120b-nsys = "bash -lc \"set -euo pipefail; mkdir -p \\\"$PIXI_PROJECT_ROOT/profiles\\\"; nsys profile -o \\\"$PIXI_PROJECT_ROOT/profiles/moe_bench_120b\\\" --trace=cuda,nvtx,osrt mojo run -I \\\"$PIXI_PROJECT_ROOT/../custom_ops\\\" \\\"$PIXI_PROJECT_ROOT/tests/mojo/bench_mxfp4_moe_ops.mojo\\\" --120b\""
profile-moe-bench-120b-tokens64-nsys = "bash -lc \"set -euo pipefail; mkdir -p \\\"$PIXI_PROJECT_ROOT/profiles\\\"; nsys profile -o \\\"$PIXI_PROJECT_ROOT/profiles/moe_bench_120b_tokens64\\\" --trace=cuda,nvtx,osrt mojo run -I \\\"$PIXI_PROJECT_ROOT/../custom_ops\\\" \\\"$PIXI_PROJECT_ROOT/tests/mojo/bench_mxfp4_moe_ops.mojo\\\" --120b --tokens64\""
profile-moe-bench-w1-ncu = "bash -lc \"set -euo pipefail; mkdir -p \\\"$PIXI_PROJECT_ROOT/profiles\\\"; sudo env MODULAR_HOME=\\\"$MODULAR_HOME\\\" /usr/local/cuda/bin/ncu --target-processes all --set basic --kernel-name-base function -k regex:kernels_moe_mxfp4_ops_moe_w1 -s 10 -c 1 --force-overwrite -o \\\"$PIXI_PROJECT_ROOT/profiles/moe_w1_ncu\\\" \\\"$PIXI_PROJECT_ROOT/.pixi/envs/default/bin/mojo\\\" run -I \\\"$PIXI_PROJECT_ROOT/../custom_ops\\\" \\\"$PIXI_PROJECT_ROOT/tests/mojo/bench_mxfp4_moe_ops.mojo\\\" --w1-only\""
profile-moe-bench-w1-tokens64-ncu = "bash -lc \"set -euo pipefail; mkdir -p \\\"$PIXI_PROJECT_ROOT/profiles\\\"; sudo env MODULAR_HOME=\\\"$MODULAR_HOME\\\" /usr/local/cuda/bin/ncu --target-processes all --set basic --kernel-name-base function -k regex:kernels_moe_mxfp4_ops_moe_w1 -s 10 -c 1 --force-overwrite -o \\\"$PIXI_PROJECT_ROOT/profiles/moe_w1_tokens64_ncu\\\" \\\"$PIXI_PROJECT_ROOT/.pixi/envs/default/bin/mojo\\\" run -I \\\"$PIXI_PROJECT_ROOT/../custom_ops\\\" \\\"$PIXI_PROJECT_ROOT/tests/mojo/bench_mxfp4_moe_ops.mojo\\\" --tokens64 --w1-only\""
profile-moe-bench-w2-ncu = "bash -lc \"set -euo pipefail; mkdir -p \\\"$PIXI_PROJECT_ROOT/profiles\\\"; sudo env MODULAR_HOME=\\\"$MODULAR_HOME\\\" /usr/local/cuda/bin/ncu --target-processes all --set basic --kernel-name-base function -k regex:kernels_moe_mxfp4_ops_moe_w2 -s 10 -c 1 --force-overwrite -o \\\"$PIXI_PROJECT_ROOT/profiles/moe_w2_ncu\\\" \\\"$PIXI_PROJECT_ROOT/.pixi/envs/default/bin/mojo\\\" run -I \\\"$PIXI_PROJECT_ROOT/../custom_ops\\\" \\\"$PIXI_PROJECT_ROOT/tests/mojo/bench_mxfp4_moe_ops.mojo\\\" --w2-only\""
profile-moe-bench-w2-tokens64-ncu = "bash -lc \"set -euo pipefail; mkdir -p \\\"$PIXI_PROJECT_ROOT/profiles\\\"; sudo env MODULAR_HOME=\\\"$MODULAR_HOME\\\" /usr/local/cuda/bin/ncu --target-processes all --set basic --kernel-name-base function -k regex:kernels_moe_mxfp4_ops_moe_w2 -s 10 -c 1 --force-overwrite -o \\\"$PIXI_PROJECT_ROOT/profiles/moe_w2_tokens64_ncu\\\" \\\"$PIXI_PROJECT_ROOT/.pixi/envs/default/bin/mojo\\\" run -I \\\"$PIXI_PROJECT_ROOT/../custom_ops\\\" \\\"$PIXI_PROJECT_ROOT/tests/mojo/bench_mxfp4_moe_ops.mojo\\\" --tokens64 --w2-only\""
format = "PYTHONPATH=$PIXI_PROJECT_ROOT ruff format $PIXI_PROJECT_ROOT/gpt_oss_mxfp4_v3"
lint = "PYTHONPATH=$PIXI_PROJECT_ROOT ruff check --fix $PIXI_PROJECT_ROOT/gpt_oss_mxfp4_v3"

[dependencies]
python = ">=3.9,<3.14"
modular = "*"
pytest = ">=8.3.3"
numpy = ">=1.26"
safetensors = ">=0.7.0,<0.8"
ruff = ">=0.14.8,<0.15"
