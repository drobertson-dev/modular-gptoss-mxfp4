[workspace]
authors = ["Modular <hello@modular.com>"]
channels = ["conda-forge", "https://conda.modular.com/max-nightly/"]
description = "An example of a custom model architecture served by MAX"
name = "Custom MAX Model Serving"
platforms = ["osx-arm64", "linux-aarch64", "linux-64"]
version = "0.1.0"

[tasks]
generate = "python -m max.entrypoints.pipelines generate --custom-architectures $PIXI_PROJECT_ROOT/gpt_oss_mxfp4 --model openai/gpt-oss-20b --max-length 10000 --prompt 'Why is the sky blue?'"
generate-smoke = "python -m max.entrypoints.pipelines generate --custom-architectures $PIXI_PROJECT_ROOT/gpt_oss_mxfp4 --model openai/gpt-oss-20b --max-length 2048 --prompt 'Say hello in one sentence.'"
generate-smoke-prof = "python -m max.entrypoints.pipelines generate --custom-architectures $PIXI_PROJECT_ROOT/gpt_oss_mxfp4 --model openai/gpt-oss-20b --max-length 2048 --prompt 'Say hello in one sentence.' --gpu-profiling detailed"
serve = "python -m max.entrypoints.pipelines serve --custom-architectures $PIXI_PROJECT_ROOT/gpt_oss_mxfp4 --model openai/gpt-oss-20b --devices gpu --chat-template $PIXI_PROJECT_ROOT/gpt_oss_mxfp4/templates/gpt-oss-20b.chat_template.jinja --device-memory-utilization 0.90 --max-batch-size 512 --max-ce-batch-size 192 --max-length 131072 --enable-in-flight-batching --experimental-background-queue --enable-chunked-prefill --prefill-chunk-size 8192 --no-enable-prefix-caching --pretty-print-config"
serve-prof = "python -m max.entrypoints.pipelines serve --custom-architectures $PIXI_PROJECT_ROOT/gpt_oss_mxfp4 --model openai/gpt-oss-20b --devices gpu --chat-template $PIXI_PROJECT_ROOT/gpt_oss_mxfp4/templates/gpt-oss-20b.chat_template.jinja --device-memory-utilization 0.90 --max-batch-size 512 --max-ce-batch-size 192 --max-length 131072 --enable-in-flight-batching --experimental-background-queue --enable-chunked-prefill --prefill-chunk-size 8192 --no-enable-prefix-caching --pretty-print-config --gpu-profiling detailed"
serve-20b = "python -m max.entrypoints.pipelines serve --custom-architectures $PIXI_PROJECT_ROOT/gpt_oss_mxfp4 --model openai/gpt-oss-20b --devices gpu --chat-template $PIXI_PROJECT_ROOT/gpt_oss_mxfp4/templates/gpt-oss-20b.chat_template.jinja --device-memory-utilization 0.90 --max-batch-size 512 --max-ce-batch-size 192 --max-length 131072 --enable-in-flight-batching --experimental-background-queue --enable-chunked-prefill --prefill-chunk-size 8192 --no-enable-prefix-caching --pretty-print-config"
serve-20b-prof = "python -m max.entrypoints.pipelines serve --custom-architectures $PIXI_PROJECT_ROOT/gpt_oss_mxfp4 --model openai/gpt-oss-20b --devices gpu --chat-template $PIXI_PROJECT_ROOT/gpt_oss_mxfp4/templates/gpt-oss-20b.chat_template.jinja --device-memory-utilization 0.90 --max-batch-size 512 --max-ce-batch-size 192 --max-length 131072 --enable-in-flight-batching --experimental-background-queue --enable-chunked-prefill --prefill-chunk-size 8192 --no-enable-prefix-caching --pretty-print-config --gpu-profiling detailed"
serve-120b = "python -m max.entrypoints.pipelines serve --custom-architectures $PIXI_PROJECT_ROOT/gpt_oss_mxfp4 --model openai/gpt-oss-120b --devices gpu --chat-template $PIXI_PROJECT_ROOT/gpt_oss_mxfp4/templates/gpt-oss-120b.chat_template.jinja --device-memory-utilization 0.95 --max-batch-size 64 --max-ce-batch-size 64 --max-length 4096 --enable-in-flight-batching --experimental-background-queue --enable-chunked-prefill --prefill-chunk-size 2048 --no-enable-prefix-caching --pretty-print-config"
serve-120b-prof = "python -m max.entrypoints.pipelines serve --custom-architectures $PIXI_PROJECT_ROOT/gpt_oss_mxfp4 --model openai/gpt-oss-120b --devices gpu --chat-template $PIXI_PROJECT_ROOT/gpt_oss_mxfp4/templates/gpt-oss-120b.chat_template.jinja --device-memory-utilization 0.95 --max-batch-size 64 --max-ce-batch-size 64 --max-length 4096 --enable-in-flight-batching --experimental-background-queue --enable-chunked-prefill --prefill-chunk-size 2048 --no-enable-prefix-caching --pretty-print-config --gpu-profiling detailed"

# NOTE: `max benchmark` expects a running `pixi run serve-*` instance.
# `SHAREGPT_PATH` defaults to `/workspace/ShareGPT_V3_unfiltered_cleaned_split.json` via the relative fallback below.
benchmark-20b-sharegpt-500-r32 = "max benchmark --model openai/gpt-oss-20b --backend modular --endpoint /v1/chat/completions --dataset-name sharegpt --num-prompts 500 --seed 0 --request-rate 32 --dataset-path ${SHAREGPT_PATH:-$PIXI_PROJECT_ROOT/../../../ShareGPT_V3_unfiltered_cleaned_split.json}"
benchmark-20b-sharegpt-500-r32-seed123 = "max benchmark --model openai/gpt-oss-20b --backend modular --endpoint /v1/chat/completions --dataset-name sharegpt --num-prompts 500 --seed 123 --request-rate 32 --dataset-path ${SHAREGPT_PATH:-$PIXI_PROJECT_ROOT/../../../ShareGPT_V3_unfiltered_cleaned_split.json}"
benchmark-20b-sharegpt-500-r16 = "max benchmark --model openai/gpt-oss-20b --backend modular --endpoint /v1/chat/completions --dataset-name sharegpt --num-prompts 500 --seed 0 --request-rate 16 --dataset-path ${SHAREGPT_PATH:-$PIXI_PROJECT_ROOT/../../../ShareGPT_V3_unfiltered_cleaned_split.json}"
benchmark-120b-sharegpt-100-r32-seed123 = "max benchmark --model openai/gpt-oss-120b --backend modular --endpoint /v1/chat/completions --dataset-name sharegpt --num-prompts 100 --seed 123 --request-rate 32 --dataset-path ${SHAREGPT_PATH:-$PIXI_PROJECT_ROOT/../../../ShareGPT_V3_unfiltered_cleaned_split.json}"
benchmark-20b-baseline = "max benchmark --model openai/gpt-oss-20b --backend modular --endpoint /v1/chat/completions --dataset-name sharegpt --num-prompts 500 --seed 123 --request-rate 32 --dataset-path ${SHAREGPT_PATH:-$PIXI_PROJECT_ROOT/../../../ShareGPT_V3_unfiltered_cleaned_split.json}"
benchmark-120b-baseline = "max benchmark --model openai/gpt-oss-120b --backend modular --endpoint /v1/chat/completions --dataset-name sharegpt --num-prompts 100 --seed 123 --request-rate 32 --dataset-path ${SHAREGPT_PATH:-$PIXI_PROJECT_ROOT/../../../ShareGPT_V3_unfiltered_cleaned_split.json}"
mxfp4-matmul-test = "PYTHONPATH=$PIXI_PROJECT_ROOT pytest $PIXI_PROJECT_ROOT/tests/test_mxfp4_matmul.py -q"
mxfp4-bootstrap-test = "PYTHONPATH=$PIXI_PROJECT_ROOT pytest $PIXI_PROJECT_ROOT/tests/test_modular_home_bootstrap.py -q"
mxfp4-moe-reference-test = "PYTHONPATH=$PIXI_PROJECT_ROOT pytest $PIXI_PROJECT_ROOT/tests/test_mxfp4_moe_reference.py -q"
mxfp4-py-tests = { depends-on = [
  "mxfp4-matmul-test",
  "mxfp4-bootstrap-test",
  "mxfp4-moe-reference-test",
] }

# Mojo tests (use Mojo runner; include `examples/custom_ops/` so `kernels.*` resolves).
mxfp4-mojo-quant-layout-test = "mojo run -I $PIXI_PROJECT_ROOT/../custom_ops $PIXI_PROJECT_ROOT/tests/mojo/test_mxfp4_quant_layout.mojo"
mxfp4-mojo-swiglu-math-test = "mojo run -I $PIXI_PROJECT_ROOT/../custom_ops $PIXI_PROJECT_ROOT/tests/mojo/test_mxfp4_swiglu_math.mojo"
mxfp4-mojo-tests = { depends-on = [
  "mxfp4-mojo-quant-layout-test",
  "mxfp4-mojo-swiglu-math-test",
] }

# SM90-only Mojo tests (expected to run only on NVIDIA sm90+; currently under active development).
mxfp4-mojo-sm90-clean-tile-test = "mojo run -I $PIXI_PROJECT_ROOT/../custom_ops $PIXI_PROJECT_ROOT/tests/mojo/test_mxfp4_sm90_clean_tile_wgmma.mojo"
mxfp4-mojo-sm90-moe-test = "mojo run -I $PIXI_PROJECT_ROOT/../custom_ops $PIXI_PROJECT_ROOT/tests/mojo/test_mxfp4_moe_sm90.mojo"
mxfp4-moe-bench = "mojo run -I $PIXI_PROJECT_ROOT/../custom_ops $PIXI_PROJECT_ROOT/tests/mojo/bench_mxfp4_moe_ops.mojo"
format = "PYTHONPATH=$PIXI_PROJECT_ROOT ruff format $PIXI_PROJECT_ROOT/gpt_oss_mxfp4"
lint = "PYTHONPATH=$PIXI_PROJECT_ROOT ruff check --fix $PIXI_PROJECT_ROOT/gpt_oss_mxfp4"

[dependencies]
python = ">=3.9,<3.14"
modular = ">=26.1.0.dev2025120907,<27"
pytest = ">=8.3.3"
numpy = ">=1.26"
safetensors = ">=0.7.0,<0.8"
ruff = ">=0.14.8,<0.15"
